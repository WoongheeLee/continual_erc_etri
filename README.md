# ì œ2íšŒ ETRI íœ´ë¨¼ì´í•´ ì¸ê³µì§€ëŠ¥ ë…¼ë¬¸ê²½ì§„ëŒ€íšŒ

## íŒ€ëª…: ì´ëª¨ì €ëª¨ ğŸ˜‚ğŸ‰

## ì—°êµ¬ ìš”ì•½
* ëŒ€í™” ì¤‘ ìë™ ê°ì • ì¸ì‹ ê¸°ìˆ ì€ ì˜ë£Œê³„ì™€ ì‚°ì—…ê³„ì˜ ë‹¤ì–‘í•œ ë¶„ì•¼ì— ì ìš©ë˜ë©° ì¤‘ìš”í•œ ì—­í• ì„ í•˜ê³  ìˆìŒ
* ê³ ì„±ëŠ¥ì˜ ìë™í™” ëœ **ë©€í‹°ëª¨ë‹¬ ë°ì´í„° ê¸°ë°˜ ê°ì • ì¸ì‹ ê¸°ìˆ  êµ¬í˜„**ì„ ìœ„í•´ì„œëŠ” **ëŒ€ê·œëª¨ì˜ í•™ìŠµ ë°ì´í„°ê°€ í•„ìš”** í•¨
* ê·¸ëŸ¬ë‚˜ ì´ëŸ¬í•œ ë°ì´í„°ëŠ” **ë¯¼ê° ì •ë³´ê°€ ë‹´ê¸´ ê²½ìš°ê°€ ë§ì•„ ì§ì ‘ì ì¸ ê³µìœ  ë³´ë‹¤ëŠ” ëª¨ë¸ íŒŒë¼ë¯¸í„°ë§Œ ê³µìœ í•˜ëŠ” ë°©ì‹**ì´ ë„ë¦¬ ì“°ì„
* ëª¨ë¸ íŒŒë¼ë¯¸í„°ë§Œì„ ì „ë‹¬ ë°›ì•„ ìƒˆë¡œìš´ íƒœìŠ¤í¬ ë°ì´í„°ë¡œ ë¯¸ì„¸ì¡°ì • í•™ìŠµ(fine-tuning)ì„ í•˜ëŠ” ê²½ìš° **ê³¼ê±° íƒœìŠ¤í¬ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ ì„±ëŠ¥ì´ ë–¨ì–´ì§€ëŠ” íŒŒê´´ì  ë§ê°(catastrophic forgetting)ì´ ë°œìƒ**
* ë”°ë¼ì„œ ì´ ì—°êµ¬ì—ì„œëŠ” **íƒœìŠ¤í¬ë³„ ì•„ë‹µí„°ë¥¼ ì‚¬ìš©í•˜ì—¬ íŒŒê´´ì  ë§ê°ì„ ë°©ì§€í•  ìˆ˜ ìˆëŠ” ë©€í‹°ëª¨ë‹¬ ê°ì • ì¸ì‹ ëª¨ë¸**ì„ êµ¬í˜„í•˜ì˜€ìŒ
## êµ¬í˜„ ë°©ë²•
* ê°ê°ì˜ ëª¨ë‹¬ë¦¬í‹°ì™€ íƒœìŠ¤í¬ì˜ íŠ¹ì§• ë²¡í„°ë¥¼ í•™ìŠµí•˜ëŠ” ì•„ë‹µí„°ë¥¼ ì‚¬ìš©í•˜ì˜€ìŒ
* ë©€í‹°ëª¨ë‹¬ ë¶„ë¥˜ ëª¨ë¸ì´ ì•„ë˜ ê·¸ë¦¼ì˜ ì™¼ìª½ (a)ì™€ ê°™ì´ ì£¼ì–´ì§
* í…ìŠ¤íŠ¸ ëª¨ë‹¬ë¦¬í‹°ì˜ íƒœìŠ¤í¬ ë³„ íŠ¹ì§• í•™ìŠµì„ ìœ„í•œ ì•„ë‹µí„° êµ¬ì¡°ëŠ” ê·¸ë¦¼ ì˜¤ë¥¸ìª½ì˜ (b), ìŒì„± ëª¨ë‹¬ë¦¬í‹°ì˜ íƒœìŠ¤í¬ ë³„ íŠ¹ì§• í•™ìŠµì„ ìœ„í•œ ì•„ë‹µí„° êµ¬ì¡°ëŠ” ê·¸ë¦¼ ì˜¤ë¥¸ìª½ì˜ (c)ì™€ ê°™ìŒ
* ì‚¬ì „í•™ìŠµ ëœ BERTì™€ Wav2Vec2ë¥¼ ì´ìš©í•˜ì—¬ ì•„ë‹µí„° ë ˆì´ì–´ë“¤ (a)ì™€ (b)ë§Œ íƒœìŠ¤í¬ë³„ í•™ìŠµì„ í†µí•´ íŒŒê´´ì  ë§ê°ì„ ì™„í™” í•¨
<p align="center">
  <img src="https://raw.githubusercontent.com/WoongheeLee/continual_erc_etri/master/figures/fig1.png" height="550"/>
</p>

## ë””ë ‰í† ë¦¬ êµ¬ì¡°
* KEMDy20ì€ v1.1
```
â”œâ”€â”€ README.md
â”œâ”€â”€ data
â”‚   â”œâ”€â”€ KEMDy19
â”‚   â””â”€â”€ KEMDy20
â”œâ”€â”€ figures
â”œâ”€â”€ models
â”œâ”€â”€ outputs
â”‚   â”œâ”€â”€ adapter
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ fold_0
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ KEMDy19
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ best_model.pt
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ KEMDy20
â”‚Â Â  â”‚Â Â  â”‚Â Â      â””â”€â”€ best_model.pt
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ fold_1
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ fold_2
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ fold_3
â”‚Â Â  â”‚Â Â  â””â”€â”€ fold_4
â”‚   â”œâ”€â”€ adapter_wo_pretraining
â”‚   â”œâ”€â”€ ewc
â”‚   â”œâ”€â”€ finetune
â”‚   â””â”€â”€ task_a
â””â”€â”€ utils
```
* ë°ì´í„° (`/data/`)
  * K. J. Noh and H. Jeong, â€œKEMDy19,â€ https://nanum.etri.re.kr/share/kjnoh/KEMDy19?lang=ko_KR 
  * K. J. Noh and H. Jeong, â€œKEMDy20,â€ https://nanum.etri.re.kr/share/kjnoh/KEMDy20?lang=ko_KR 
* ì²´í¬í¬ì¸íŠ¸ (`/outputs/`)
  * [ë§í¬](https://hyu-my.sharepoint.com/:f:/g/personal/onnoo_hanyang_ac_kr/EoevacD34iBOsz7w2J3bMqQBoSOIfZAN5tD6vqOTRs3NTw?e=wacat9)

## ì‚¬ìš© ì„¤ëª…ì„œ
### 1) ì‹¤í—˜ í™˜ê²½ :sparkling_heart:
* Python 3.8
* pytorch==1.13.1
* torchaudio==0.13.1
* install dependencies : `pip install -r requirements.txt`

### 2ï¸) ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ìˆœì„œ :tada:

```
bash run-init.sh
bash run-finetune.sh
bash run-ewc.sh
bash run-adapter.sh
bash run-adapter-wo-pretraining.sh
```

### 3) ì„±ëŠ¥ ë¹„êµ :fire:

ìœ„ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ìˆœì„œì— ìˆœì„œì— ë”°ë¼ í•™ìŠµì´ ì™„ë£Œ ëœ í›„ [result.ipynb](https://github.com/WoongheeLee/continual_erc_etri/blob/master/result.ipynb)ì—ì„œ catastrophic forgetting ì™„í™” ì„±ëŠ¥ì„ í™•ì¸ ê°€ëŠ¥

|   | task_a(original) | finetune | ewc | adapter | adapter_wo_pretrain |
|---|---|---|---|---|---|
| **KEMDy19** | 0.70389 | 0.524502 | 0.599862 | 0.681204 | 0.510747 |
| **KEMDy20** | 0.87122 | 0.765711 | 0.761987 | 0.847727 | 0.833978 |

#### íŒŒê´´ì  ë§ê° ë°©ì§€ ì„±ëŠ¥ ë¹„êµ
<img src="https://raw.githubusercontent.com/WoongheeLee/continual_erc_etri/master/figures/fig2.png" width="700"/>

#### ì‚¬ì „í•™ìŠµ ì—¬ë¶€ì— ë”°ë¥¸ í˜¼ë™ í–‰ë ¬
<img src="https://raw.githubusercontent.com/WoongheeLee/continual_erc_etri/master/figures/fig3.png" width="700"/>

## Reference
* Houlsby, Neil, et al. "Parameter-efficient transfer learning for NLP." International Conference on Machine Learning. *PMLR*, 2019.
* Kirkpatrick, James, et al. "Overcoming catastrophic forgetting in neural networks." *Proceedings of the national academy of sciences* 114.13 (2017): 3521-3526.
